{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b501472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99caffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"albert-xxlarge-v2\"\n",
    "\n",
    "# model_name = \"roberta_large\"\n",
    "# mask_name = \"<mask>\"\n",
    "\n",
    "model_name = \"bert-large-cased\"\n",
    "mask_name = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e917a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pair = \"eng_ron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d79ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6443120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path = f\"/mounts/work/akoksal/word_alignment_silver/{lang_pair}/{lang_pair}.txt\"\n",
    "parallel_path = f\"/mounts/work/akoksal/word_alignment_silver/{lang_pair}/parallel.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a7b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gold_path, \"r\") as f:\n",
    "    gold_content = [x.split(\"|||\")[0].strip() for x in f.read().splitlines()]\n",
    "\n",
    "with open(parallel_path, \"r\") as f:\n",
    "    parallel_content = [x.split(\"|||\")[0].strip() for x in f.read().splitlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4578aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "parallel_sample_content = random.sample(parallel_content, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fae360bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 1000, 39101)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_content), len(parallel_sample_content), len(parallel_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca3a57",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "105fd193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "aug_tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/mounts/work/akoksal/hf_cache\")\n",
    "aug_model = AutoModelForMaskedLM.from_pretrained(model_name, cache_dir=\"/mounts/work/akoksal/hf_cache\").to(device)\n",
    "aug_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6523f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_sentences(target_data):\n",
    "    final_sentences = []\n",
    "    \n",
    "    for sentence_id, sentence in tqdm(enumerate(target_data)):\n",
    "        words = sentence.split()\n",
    "        for word_id in range(len(words)):\n",
    "            res = {\"sentence_id\": sentence_id, \"word_id\":word_id, \"word\":words[word_id]}\n",
    "            new_words = words[:word_id]+[aug_tokenizer.mask_token]+words[word_id+1:]\n",
    "            masked_sentence = \" \".join(new_words)\n",
    "            res[\"masked_sentence\"] = masked_sentence\n",
    "\n",
    "            inputs = aug_tokenizer(res[\"masked_sentence\"], return_tensors=\"pt\").to(device)\n",
    "            outputs = aug_model(**inputs)\n",
    "\n",
    "            masked_id_loc = [idx for idx, el in enumerate(inputs[\"input_ids\"][0]) if el==aug_tokenizer.mask_token_id][0]\n",
    "            vals = outputs[\"logits\"].detach().to(\"cpu\")[0][masked_id_loc]\n",
    "            probs = torch.nn.Softmax(dim=0)(vals)\n",
    "            word = \" \"+res[\"word\"]\n",
    "\n",
    "            prob = probs[aug_tokenizer.encode(word, add_special_tokens=False)]\n",
    "            if len(prob)==1:\n",
    "                res[\"one_token\"] = True\n",
    "                res[\"prob\"] = round(float(probs[aug_tokenizer.encode(word, add_special_tokens=False)]), 3)\n",
    "            else:\n",
    "                res[\"one_token\"] = False\n",
    "            possible_vals = []\n",
    "            counter = 0\n",
    "            added_words = set()\n",
    "            for el in reversed(probs.argsort()[-10:]):\n",
    "                tword = aug_tokenizer.decode([el]).strip()\n",
    "#                 print(tword, word)\n",
    "                if tword.startswith(\"##\"):\n",
    "                    continue\n",
    "                if tword.lower() in added_words:\n",
    "                    continue\n",
    "                if tword == word.strip():\n",
    "                    possible_vals.append({\"prob\":round(float(probs[el]), 3), \"word\":tword,\n",
    "                                         \"token\":int(el)})\n",
    "                    added_words.add(tword.lower())\n",
    "                    counter += 1\n",
    "                elif tword.lower()==word.strip().lower():\n",
    "                    continue\n",
    "                else:\n",
    "                    possible_vals.append({\"prob\":round(float(probs[el]), 3), \"word\":tword,\n",
    "                                         \"token\":int(el)})\n",
    "                    added_words.add(tword.lower())\n",
    "                    counter += 1\n",
    "                    \n",
    "                if counter == 5:\n",
    "                    break\n",
    "            res[\"replacement\"] = possible_vals\n",
    "\n",
    "            final_sentences.append(res)\n",
    "    return final_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f17aaa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [02:02,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sentences_gold = augmented_sentences(gold_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "160f2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/mounts/work/akoksal/word_alignment_silver/{lang_pair}/gold_augmented.json\", \"w\") as f:\n",
    "    json.dump(final_sentences_gold, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6acf171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [09:59,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sentences_parallel = augmented_sentences(parallel_sample_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05aefa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/mounts/work/akoksal/word_alignment_silver/{lang_pair}/parallel_sample_augmented.json\", \"w\") as f:\n",
    "    json.dump(final_sentences_parallel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4ae34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d03d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lmbias] *",
   "language": "python",
   "name": "conda-env-lmbias-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
